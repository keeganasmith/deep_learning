#!/bin/bash
#SBATCH --job-name=transformer
#SBATCH --time=2-00:00:00
#SBATCH --nodes=2                      # now use 2 nodes
#SBATCH --ntasks-per-node=2            # 1 task per GPU
#SBATCH --cpus-per-task=96             # 192 cores/node ÷ 2 tasks = 96
#SBATCH --exclusive
#SBATCH --partition=gpu
#SBATCH --gres=gpu:a30:2               # 2 GPUs per node
#SBATCH --mem=371G
#SBATCH --output=transformer_out.%j    # include jobid in output

module purge
cd /scratch/user/u.ks124812/CSCE636/project/
source modules.sh
source venv/bin/activate
cd training

#–– Rendezvous & NCCL tuning ––
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n1)
export MASTER_PORT=12355                # any free port
export WORLD_SIZE=$SLURM_NTASKS        # total processes = nodes * tasks/node = 4
export RANK=$SLURM_PROCID              # 0…WORLD_SIZE-1
export LOCAL_RANK=$SLURM_LOCALID       # 0…nproc_per_node-1

#–– Launch DDP training ––
srun python train_ddp.py \
     --epochs 50 \
     --lr 2e-4 \
     --batch_size 512
